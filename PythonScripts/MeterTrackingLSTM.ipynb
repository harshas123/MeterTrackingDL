{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meter Tracking in Carnatic Music"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "In this project, we are trying to track the beats in Carnatic music using a deep learning based on Long Short-Term Memory (LSTM) networks. The idea is to use raw T-F feautres from audio data and from this get instances where a beat should occur using LSTM network. \n",
    "\n",
    "## Dataset Details\n",
    "Ajay's hand labeled dataset - CMR_dataset_full. The dataset contains:\n",
    "1. 176 audio files\n",
    "2. Beat-level annotations \"1-8\" with their time stamp in seconds for each of the audio files\n",
    "3. Meter-level annotations (not used currently)\n",
    "\n",
    "### Data Partitioning\n",
    "The Entire audio data is divided into:\n",
    "1. Training Data\n",
    "2. Train-Dev Data: Same distribution as Training Data, but not used in Training.\n",
    "3. Dev Data: Part of the data has same distribution as training and a part has different distribution. \n",
    "4. Test Data: Same as Dev data.\n",
    "\n",
    "The way I partition the data is as follows:\n",
    "1. The entire data is first divided into two sets - Set-1: 167 files, Set-2: 9 files\n",
    "2. Training, Train-Dev and a part of Dev and Test data are chosen from Set -1 in the ratio:\n",
    "\n",
    "    a) Training - 60%\n",
    "    \n",
    "    b) Train-Dev - 10%\n",
    "\n",
    "    c) Dev - 15%\n",
    "\n",
    "    d) Test - 15%\n",
    "    \n",
    "Note: The partitioning here is such that each partition has feature vectors from all 167 audio clips. \n",
    "3. Additionally the reamining part of the Dev and Test data are chosen from Set-2 in the ratio:\n",
    "    \n",
    "    a) Dev - 15%\n",
    "    b) Test - 15%\n",
    "    c) Train - 70% (Note this is part of the data is currently not used....for future if we observe that the model is not generalizing well to Dev data, this can be used)\n",
    "    \n",
    "This kind of partitioning ensures, Training and Train-Dev data have the same distribution. A part of Dev and Test Data have the same distribution as Training Data and the rest have a different distribution.\n",
    "    \n",
    "## Input features\n",
    "----Previously-----\n",
    "1. Auditory spectrogram features - 128 dimensional features obtained from a mammalian auditory model.\n",
    "2. Window length = 50ms (I had a feeling that for Music window length must be bigger than speech. Starting out with this big window...)\n",
    "3. Window Shift = 100ms. (In auditory spectrogram, typically non-overalpping features are used.)\n",
    "\n",
    "----Current------\n",
    "1. Log Mel spectrogram features - 80 dimensional features obtained from HTK.\n",
    "2. Window length = 46.44ms \n",
    "3. Window Shift = 10ms. \n",
    "\n",
    "## Labels\n",
    "---Previously---\n",
    "The timing specified in the *.beats file is converted to frame numbers. Frames containing a beat -->1 else -->0\n",
    "\n",
    "---Currently----\n",
    "The above way of generating labels gave rise to class imbalance problem...so solving this by smoothing...\n",
    "The same as above with gaussian smoothing. So, labels are no longer binary but are logistic. \n",
    "\n",
    "## Network Architecture\n",
    "\n",
    "A crude idea is to use a bi-directional LSTM network for classifying each frame of input data into two classes - \"Beat\" vs \"No Beat\". Trying initially with 3 layers each for forward and backward layers with 25 units each. \n",
    "1. Input - 128 dimensional\n",
    "2. Output (Softmax/Logistic) - 2 dimensional (One-Hot encoding) \n",
    "3. Number of hidden Layers = 3 (forward) + 3(backward)\n",
    "\n",
    "\n",
    "## Notations and Convention\n",
    "1. Number of examples - m\n",
    "2. Input Dimensions - n_input\n",
    "3. Number of time steps for TBPTT for training LSTMs = n_steps\n",
    "4. Number of output classes - n_classes\n",
    "5. Number of LSTMs per layer - num_hidden\n",
    "6. Number of hidden Layers - number_of_layers\n",
    "\n",
    "## Code Pipeline\n",
    "1. Feature Processing + Partitioning - MATLAB (../MatlabScripts/DataPrep.m): This gives out 4 .mat files in the \"../Data/\" folder:\n",
    "\n",
    "    a) Train.mat - Training data of size DIM X m_train\n",
    "    \n",
    "    b) Train-Dev.mat - Train-dev data of size DIM X m_train_dev\n",
    "    \n",
    "    c) Dev.mat - Dev data of size DIM X m_dev\n",
    "    \n",
    "    d) Test.mat - Test data of size DIM X m_test\n",
    "    \n",
    "    where DIM = 80 for log-melspec features and 128 for suditory filterbank features.\n",
    "    \n",
    "This code expects audio data in: \"../Database/CMR_dataset_full/audio/\" and beat annotations in \"../Database/CMR_dataset_full/annotations/beats/\".\n",
    "    \n",
    "2. The LSTM implementation is in this IPython notebook using Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "#Import the required packages\n",
    "import numpy as np\n",
    "import math\n",
    "import h5py\n",
    "import scipy.io as sio\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "print tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all the required Functions\n",
    "def mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if No-beat, 1 beat), of shape (2, number of examples - one-hot encoding)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- to reproduce results later...\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    n_input, m = X.shape                  # number of examples\n",
    "    n_output = Y.shape[0]\n",
    "    \n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = int(math.floor(m/mini_batch_size)) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    batch_X = np.zeros((num_complete_minibatches,mini_batch_size, n_input))\n",
    "    batch_Y = np.zeros((num_complete_minibatches,mini_batch_size, n_output))\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        batch_X[k,:,:] = mini_batch_X.T\n",
    "        batch_Y[k,:,:] = mini_batch_Y.T\n",
    "   \n",
    "    #Shuffle the batches before you send it across\n",
    "    permutation = list(np.random.permutation(num_complete_minibatches))\n",
    "    shuffled_batch_X = batch_X[permutation,:,:]\n",
    "    shuffled_batch_Y = batch_Y[permutation,:,:]\n",
    "\n",
    "    return shuffled_batch_X, shuffled_batch_Y\n",
    "    \n",
    "def read_from_h5py_mat(dataset_type):\n",
    "    # Inputs: dataset_type - Name of the Partition. Should be one of:\n",
    "    # \"Train\", \"Train-Dev\", \"Dev\", or \"Test\"\n",
    "    \n",
    "    fname = \"../Data/\" + dataset_type + \".mat\"\n",
    "    varname = dataset_type.replace('-','_').lower()\n",
    "    f = h5py.File(fname, 'r')\n",
    "    data_X = np.array(f.get(varname +\"_X\")).T\n",
    "    data_Y = np.array(f.get(varname +\"_Y\")).T\n",
    "    #print data_Y.shape\n",
    "    return data_X, data_Y\n",
    "    \n",
    "def load_music_dataset(mini_batch_size):\n",
    "    \n",
    "    \n",
    "    train_X, train_Y = read_from_h5py_mat(\"Train\")\n",
    "    train_dev_X, train_dev_Y = read_from_h5py_mat(\"Train-Dev\")\n",
    "    dev_X, dev_Y = read_from_h5py_mat(\"Dev\")\n",
    "    test_X, test_Y = read_from_h5py_mat(\"Test\")\n",
    "    \n",
    "    \n",
    "    #Try taking the difference of successive frames to get better features for beat detection\n",
    "#    train_X = np.diff(train_X, axis=1)\n",
    "#    train_Y = np.diff(train_Y, axis=1)\n",
    "#    \n",
    "#    train_dev_X = np.diff(train_dev_X, axis=1)\n",
    "#    train_dev_Y = np.diff(train_dev_Y, axis=1)\n",
    "#    \n",
    "#    dev_X = np.diff(dev_X, axis=1)\n",
    "#    dev_Y = np.diff(dev_Y, axis=1)\n",
    "#    \n",
    "#    test_X = np.diff(test_X, axis=1)\n",
    "#    test_Y = np.diff(test_Y, axis=1)\n",
    "    \n",
    "#    # Normalize Data to have zero mean and unit variance along all dimensions    \n",
    "#    mean_vec = np.mean(train_X, axis=1, keepdims=True)\n",
    "#    train_X = train_X - mean_vec\n",
    "#    std_vec = np.std(train_X, axis=1, keepdims=True)\n",
    "#    print std_vec\n",
    "#    train_X /= std_vec\n",
    "#    train_Y = np.reshape(train_Y[1,:],(1,-1))\n",
    "    #Reshape data in train_X to fit Tensorflow RNN cell requirement to be of \n",
    "    #size - (batch_size, n_steps, n_input)\n",
    "    #\n",
    "    train_X, train_Y = mini_batches(train_X, train_Y, mini_batch_size)\n",
    "\n",
    "    #Apply Training normalization to Train-Dev dataset\n",
    "#    train_dev_X = train_dev_X - mean_vec\n",
    "#    train_dev_X /= std_vec\n",
    "#    train_dev_Y = np.reshape(train_dev_Y[1,:],(1,-1))\n",
    "    #Reshape data for Tensorflow and divide into batches\n",
    "    train_dev_X, train_dev_Y = mini_batches(train_dev_X, train_dev_Y, mini_batch_size)\n",
    "\n",
    "    #Apply Training normalization to Dev dataset\n",
    "#    dev_X = dev_X - mean_vec\n",
    "#    dev_X /= std_vec\n",
    "#    dev_Y = np.reshape(dev_Y[1,:],(1,-1))\n",
    "    #Reshape data for Tensorflow and divide into batches\n",
    "    dev_X, dev_Y = mini_batches(dev_X, dev_Y, mini_batch_size)\n",
    "\n",
    "    #Apply Training normalization to Dev dataset\n",
    "#    test_X = test_X - mean_vec\n",
    "#    test_X /= std_vec\n",
    "#    test_Y = np.reshape(test_Y[1,:],(1,-1))\n",
    "    #Reshape data for Tensorflow and divide into batches\n",
    "    test_X, test_Y = mini_batches(test_X, test_Y, mini_batch_size)\n",
    "\n",
    "    \n",
    "    return train_X, train_Y, train_dev_X, train_dev_Y, dev_X, dev_Y, test_X, test_Y\n",
    "\n",
    "def create_placeholders(n_input, n_classes, n_steps):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_input -- scalar, Dimension of the input features (here it is 128)\n",
    "    n_classes -- scalar, number of output classes (=2, beat/no beat classification)\n",
    "    n_steps -- scalar, The number time steps in of Truncated BackProp Through Time (TBPTT)\n",
    "    \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_input, n_steps, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [n_classes, n_steps, None] and dtype \"float\"\n",
    "    \"\"\"\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_steps, n_input), name=\"Input\")\n",
    "    Y = tf.placeholder(tf.float32, shape=(None, n_steps, n_classes), name=\"Labels\")\n",
    "    \n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "def initialize_parameters(num_hidden, n_classes):\n",
    "    \"\"\"\n",
    "    Initializes parameters of the LSTM network. The shapes are:\n",
    "                        \n",
    "    Input: \n",
    "    num_hidden -- scalar, indicates the number of LSTMS in the final hidden layer\n",
    "    n_classes -- scalar, number of output classes (=2)\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W-out b-out, Weights and bias for the Softmax layer.\n",
    "    Note: The other weights are implicitly taken care of by Tensorflow.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Initialize the Weights and Biases of the final Softmax Layer\n",
    "\n",
    "    parameters = {}\n",
    "    parameters[\"W-out\"] = tf.get_variable(\"W-out\", shape=[2*num_hidden, n_classes], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    parameters[\"b-out\"] = tf.get_variable(\"b-out\", shape=[n_classes], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    #Note: I am multiplying 2 to num_hidden in parameters[\"W-out\"], because this will be a bi-directional LSTM\n",
    "    #with num_hidden units for forward LSTM layer and num_hidden units for backward LSTM layer.\n",
    "    \n",
    "    return parameters\n",
    "    \n",
    "def RNN(x, parameters, n_steps, num_hidden, number_of_layers, n_classes):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (num_batches, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (num_batches, n_input)\n",
    "    #n_steps = x.shape[1]\n",
    "    # Permuting num_batches and n_steps\n",
    "    x = tf.transpose(x, [1, 0, 2])\n",
    "    # Reshaping to (n_steps*num_batches, n_input)\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "    # Split to get a list of 'n_steps' tensors of shape (num_batches, n_input)\n",
    "    x = tf.split(x, n_steps, axis=0)\n",
    "    \n",
    "    \n",
    "    # Define a lstm cell with tensorflow\n",
    "    ########NOTE to Self: Re-Check stat_is_tuple and confirm concretely\n",
    "    \n",
    "    stacked_lstm_fw=[]\n",
    "    stacked_lstm_bw=[]\n",
    "    for layer_id in range(number_of_layers):\n",
    "        stacked_lstm_fw.append(rnn.LSTMCell(num_hidden, state_is_tuple=True))\n",
    "        stacked_lstm_bw.append(rnn.LSTMCell(num_hidden, state_is_tuple=True))\n",
    "    multi_fw = rnn.MultiRNNCell(stacked_lstm_fw, state_is_tuple=True)\n",
    "    multi_bw = rnn.MultiRNNCell(stacked_lstm_bw, state_is_tuple=True)\n",
    "\n",
    "    outputs,_,_  = rnn.static_bidirectional_rnn(multi_fw, multi_bw, x, dtype=tf.float32)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    #outputs - n_steps X num_batches X n_input\n",
    "    #Permuting the first and second dimensions - num_batches X n_steps X n_input\n",
    "    op2 = tf.transpose(outputs,[1,0,2])\n",
    "    # Reshaping to 2D tensor for multiplication (num_batches*n_steps) X num_hidden\n",
    "    op3 = tf.reshape(op2,[-1,2*num_hidden])\n",
    "    \n",
    "    #Return the logits of shape num_batches X n_steps X n_classes\n",
    "    logits = tf.reshape(tf.matmul(op3, parameters[\"W-out\"]) + parameters[\"b-out\"],[-1,n_steps,n_classes], name=\"logits\")\n",
    "    \n",
    "    return logits\n",
    "\n",
    "def compute_cost(logits,Y):\n",
    "    \n",
    "    #Both logits and Y are of shape num_batches X n_steps X n_classes. So, convert them to be \n",
    "    # of shape (batch_size X n_steps) X n_classes\n",
    "    logits = tf.reshape(logits, [-1, n_classes])\n",
    "    labels = tf.reshape(Y, [-1, n_classes])\n",
    "    #weights = tf.reshape(tf.constant([0.1,0.9]),[1,2])\n",
    "    #logits = logits*weights\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    #cost = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits\\\n",
    "    #                    (logits=logits, targets=labels,pos_weight=0.010), name=\"Cost\")\n",
    "    \n",
    "    return cost\n",
    "    \n",
    "def model(train_X, train_Y, train_dev_X, train_dev_Y, dev_X, dev_Y, test_X, test_Y,\\\n",
    "          num_hidden, num_layers, learning_rate = 0.0001, num_epochs = 1500, \\\n",
    "          minibatch_size = 32, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a Multi-layer tensorflow LSTM Recurrent neural network: \n",
    "        LSTM * num_layers ->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    train_X -- training set, of shape (input size = 128, number of training examples = ?)\n",
    "    train_Y -- training labels, of shape (output size = 1, number of training examples = ?)\n",
    "    train_dev_X -- train-dev set, of shape (input size = 128, number of examples = ?)\n",
    "    train_dev_Y -- train-dev labels, of shape (output size = 1, number of examples = ?)\n",
    "    dev_X -- dev set, of shape (input size = 128, number of examples = ?)\n",
    "    dev_Y -- dev labels, of shape (output size = 1, number of examples = ?)\n",
    "    test_X -- test set, of shape (input size = 128, number of examples = ?)\n",
    "    test_Y -- test labels, of shape (output size = 1, number of examples = ?)\n",
    "    num_layers -- Number of LSTM layers\n",
    "    num_hidden -- Number of LSTM cells per layer\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch == Number of steps = n_steps == \n",
    "    Number of steps in a truncated back prop algo\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    tf.reset_default_graph()\n",
    "    _, n_steps, n_input = train_X.shape\n",
    "    n_classes = train_Y.shape[2]\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders for X and Y\n",
    "    X, Y = create_placeholders(n_input, n_classes, n_steps)\n",
    "\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(num_hidden, n_classes)\n",
    "\n",
    "    \n",
    "    # Forward propagation: Build the RNN based forward propagation in the tensorflow graph\n",
    "    logits = RNN(X, parameters, n_steps, num_hidden, number_of_layers, n_classes)\n",
    "\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(logits, Y)\n",
    "\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "     # Calculate the correct predictions: Compute the average Hamming Score\n",
    "#    nn_op =  \n",
    "    logits = tf.reshape(logits,[-1, n_classes])\n",
    "    labels = tf.reshape(Y,[-1, n_classes])\n",
    "    pred = tf.nn.softmax(logits)\n",
    "    #correct_prediction = tf.equal(tf.argmax(pred,axis=1), tf.argmax(labels,axis=1))\n",
    "    #correct_prediction = tf.equal(pred >= 0.5, labels>0)\n",
    "\n",
    "    # Calculate accuracy on the test set\n",
    "    #accuracy = tf.reduce_mean(tf.reduce_mean(tf.cast(correct_prediction, \"float\"),axis=1))*100.0\n",
    "    #accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))*100.0\n",
    "    #Do not use Accuracy use F-measure or F1 score\n",
    "    #_,prec = tf.metrics.precision(predictions=pred, labels=labels)\n",
    "    #_,recall = tf.metrics.recall(predictions=pred, labels=labels)\n",
    "    #accuracy = 2.0 * tf.multiply(prec,recall)/(tf.add(prec, recall))#Actually F-score but just called accuracy\n",
    "    _,accuracy = tf.metrics.auc(predictions=pred, labels=labels)\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            \n",
    "            # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "            \n",
    "            _ , batch_cost = sess.run([optimizer, cost], feed_dict={X: train_X, Y: train_Y})\n",
    "            \n",
    "            \n",
    "            epoch_cost += batch_cost\n",
    "\n",
    "            # Print the cost every epoch \n",
    "            if print_cost == True :\n",
    "                #parameters = sess.run(parameters)\n",
    "                train_acc = sess.run(accuracy, feed_dict={X: train_X, Y: train_Y})\n",
    "                train_dev_acc = sess.run(accuracy, feed_dict={X: train_dev_X, Y: train_dev_Y})\n",
    "                dev_acc = sess.run(accuracy, feed_dict={X: dev_X, Y: dev_Y})\n",
    "                print (\"%i\\t %.5f\\t %.2f\\t %.2f\\t %.2f\\n \" % (epoch, epoch_cost, train_acc, train_dev_acc, dev_acc))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        train_acc = sess.run(accuracy, feed_dict={X: train_X, Y: train_Y})\n",
    "        train_dev_acc = sess.run(accuracy, feed_dict={X: train_dev_X, Y: train_dev_Y})\n",
    "        dev_acc = sess.run(accuracy, feed_dict={X: dev_X, Y: dev_Y})\n",
    "        test_acc = sess.run(accuracy, feed_dict={X: test_X, Y: test_Y})\n",
    "        \n",
    "        train_pred = pred.eval({X: train_X, Y: train_Y})\n",
    "        train_dev_pred = pred.eval({X: train_dev_X, Y: train_dev_Y})\n",
    "        dev_pred = pred.eval({X: dev_X, Y: dev_Y})\n",
    "        test_pred = pred.eval({X: test_X, Y: test_Y})\n",
    "\n",
    "        \n",
    "        \n",
    "        return parameters, costs, train_acc, train_dev_acc, dev_acc, test_acc,\\\n",
    "                                train_pred, train_dev_pred, dev_pred, test_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "Note the different partitions will be of shape - num_batches X n_steps X n_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data shape = (34405, 100, 80)\n",
      "Train-Dev Data shape = (5734, 100, 80)\n",
      "Dev Data shape = (8992, 100, 80)\n",
      "Test Data shape = (8991, 100, 80)\n",
      "Train Labels shape = (34405, 100, 2)\n",
      "Train-Dev Labels shape = (5734, 100, 2)\n",
      "Dev Labels shape = (8992, 100, 2)\n",
      "Test Labels shape = (8991, 100, 2)\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "n_steps = 100\n",
    "train_X, train_Y, train_dev_X, train_dev_Y, dev_X, dev_Y, test_X, test_Y = load_music_dataset(n_steps)\n",
    "print(\"Train Data shape = \" + str(train_X.shape))\n",
    "print(\"Train-Dev Data shape = \" + str(train_dev_X.shape))\n",
    "print(\"Dev Data shape = \" + str(dev_X.shape))\n",
    "print(\"Test Data shape = \" + str(test_X.shape))\n",
    "print(\"Train Labels shape = \" + str(train_Y.shape))\n",
    "print(\"Train-Dev Labels shape = \" + str(train_dev_Y.shape))\n",
    "print(\"Dev Labels shape = \" + str(dev_Y.shape))\n",
    "print(\"Test Labels shape = \" + str(test_Y.shape))\n",
    "print(\"---------------------------------------------------------------\")\n",
    "print(\"---------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Network and Save the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch\tEpoch-Cost\tTrain-Acc\tTrainDev-Acc\tDev-Acc\n",
      "0\t 0.66681\t 0.80\t 0.80\t 0.80\n",
      " \n",
      "1\t 0.64653\t 0.83\t 0.84\t 0.84\n",
      " \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9edf9fafc6df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnew_params_fname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../Results/Parameters_NN_config_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnn_config\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".mat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch\\tEpoch-Cost\\tTrain-Acc\\tTrainDev-Acc\\tDev-Acc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dev_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m                    \u001b[0mtrain_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dev_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dev_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dev_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_Y\u001b[0m\u001b[0;34m,\u001b[0m                       \u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_Y\u001b[0m\u001b[0;34m,\u001b[0m                       \u001b[0mnum_hidden\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnum_hidden\u001b[0m\u001b[0;34m,\u001b[0m                       \u001b[0mnum_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumber_of_layers\u001b[0m\u001b[0;34m,\u001b[0m                       \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m                        \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_iter\u001b[0m\u001b[0;34m,\u001b[0m                        \u001b[0mminibatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavemat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_params_fname\u001b[0m\u001b[0;34m,\u001b[0m            \u001b[0;34m{\u001b[0m\u001b[0;34m\"parameters\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'costs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcosts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_acc'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m             \u001b[0;34m'train_dev_acc'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_dev_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dev_acc'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdev_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test_acc'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m            \u001b[0;34m'train_pred'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_dev_pred'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_dev_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dev_pred'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdev_pred\u001b[0m\u001b[0;34m,\u001b[0m            \u001b[0;34m'test_pred'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_Y'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_dev_Y'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_dev_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dev_Y'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdev_Y\u001b[0m\u001b[0;34m,\u001b[0m            \u001b[0;34m'test_Y'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest_Y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-094cdb37af50>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(train_X, train_Y, train_dev_X, train_dev_Y, dev_X, dev_Y, test_X, test_Y, num_hidden, num_layers, learning_rate, num_epochs, minibatch_size, print_cost)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;31m# Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0m_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbatch_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_input = train_X.shape[2]\n",
    "num_hidden = 25\n",
    "number_of_layers = 3\n",
    "n_classes = train_Y.shape[2]\n",
    "num_iter = 10\n",
    "nn_config = \"NumLayers_\" + str(number_of_layers) + \"num_iter_\" + str(num_iter)\n",
    "\n",
    "new_params_fname = '../Results/Parameters_NN_config_' + nn_config + \".mat\"\n",
    "print(\"Epoch\\tEpoch-Cost\\tTrain-Acc\\tTrainDev-Acc\\tDev-Acc\")\n",
    "parameters, costs, train_acc, train_dev_acc, dev_acc, test_acc,\\\n",
    "                    train_pred, train_dev_pred, dev_pred, test_pred =\\\n",
    "                 model(train_X, train_Y, train_dev_X, train_dev_Y, dev_X, dev_Y,\\\n",
    "                       test_X, test_Y,\\\n",
    "                       num_hidden= num_hidden,\\\n",
    "                       num_layers = number_of_layers,\\\n",
    "                       learning_rate = 0.0001, \\\n",
    "                       num_epochs = num_iter, \\\n",
    "                       minibatch_size = n_steps)\n",
    "\n",
    "sio.savemat(new_params_fname,\\\n",
    "            {\"parameters\":parameters, 'costs':costs, 'train_acc': train_acc, \\\n",
    "            'train_dev_acc': train_dev_acc, 'dev_acc': dev_acc, 'test_acc': test_acc,\\\n",
    "            'train_pred':train_pred, 'train_dev_pred':train_dev_pred, 'dev_pred':dev_pred,\\\n",
    "            'test_pred':test_pred, 'train_Y':train_Y, 'train_dev_Y':train_dev_Y, 'dev_Y':dev_Y,\\\n",
    "            'test_Y':test_Y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Comments:\n",
    "\n",
    "1. Becuase of the huge data imbalance, the network is currently learning to only output zeros (mostly).\n",
    "2. Weighted Cross-Entropy is somehow not heling much...\n",
    "3. Need to figure out a way to train the network properly with data imbalance. I feel we can not emply data resampling techniques to either oversample the minority class or undersample the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print parameters[\"W-out\"].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
